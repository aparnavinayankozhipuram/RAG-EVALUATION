# -*- coding: utf-8 -*-
"""9 APRIL - RAG- Retrieve, Augment and Generate results

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xKXPzC_MIItfg8PY7FCZIzWNSyC5rC0s
"""

!pip install langchain
!pip install torch
!pip install sentence_transformers
!pip install faiss-cpu
!pip install huggingface-hub
!pip install pypdf
!pip -q install accelerate
!pip install llama-cpp-python
!pip -q install git+https://github.com/huggingface/transformers
!pip install -U langchain-community
!pip install -q -U langchain transformers bitsandbytes accelerate
!pip install transformers
!pip install langchain
!pip install torch
!pip install sentence_transformers
!pip install faiss-cpu
!pip install huggingface-hub
!pip install pypdf
!pip -q install accelerate
!pip install llama-cpp-python
!pip -q install git+https://github.com/huggingface/transformers
!pip install -U langchain-community
!pip install -q -U langchain transformers bitsandbytes accelerate
!pip install transformers
!pip install --upgrade transformers
!pip install -q -U langchain transformers bitsandbytes accelerate
!pip install langchain-community
!pip install transformers accelerate bitsandbytes
!pip install langchain chromadb langchain_community pypdf
!pip install -U -q langchain openai ragas arxiv pymupdf chromadb wandb tiktoken

from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import LlamaCpp
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFDirectoryLoader
import torch
from transformers import BitsAndBytesConfig
from langchain import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from transformers import GenerationConfig
from langchain.llms import HuggingFacePipeline
from transformers import GenerationConfig
from langchain import PromptTemplate
from langchain.llms import HuggingFacePipeline  # Importing HuggingFacePipeline
import torch
from transformers import BitsAndBytesConfig
from langchain import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, GenerationConfig
import torch

if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

#from langchain.document_loaders import ArxivLoader

#base_docs = ArxivLoader(query="Retrieval Augmented Generation", load_max_docs=1).load()
#len(base_docs)

#base_docs

# Download PDF file
import os
import requests

# Get PDF document
pdf_path = "TAX.pdf"

# Download PDF if it doesn't already exist
if not os.path.exists(pdf_path):
  print("File doesn't exist, downloading...")

  # The URL of the PDF you want to download
  url = "https://ejournal.wiraraja.ac.id/index.php/FEB/article/view/4168/2400"

  # The local filename to save the downloaded file
  filename = pdf_path

  # Send a GET request to the URL
  response = requests.get(url)

  # Check if the request was successful
  if response.status_code == 200:
      # Open a file in binary write mode and save the content to it
      with open(filename, "wb") as file:
          file.write(response.content)
      print(f"The file has been downloaded and saved as {filename}")
  else:
      print(f"Failed to download the file. Status code: {response.status_code}")
else:
  print(f"File {pdf_path} exists.")

# Requires !pip install PyMuPDF, see: https://github.com/pymupdf/pymupdf
import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)
from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm

def text_formatter(text: str) -> str:
    """Performs minor formatting on text."""
    cleaned_text = text.replace("\n", " ").strip() # note: this might be different for each doc (best to experiment)

    # Other potential text formatting functions can go here
    return cleaned_text

# Open PDF and get lines/pages
# Note: this only focuses on text, rather than images/figures etc
def open_and_read_pdf(pdf_path: str) -> list[dict]:
    """
    Opens a PDF file, reads its text content page by page, and collects statistics.

    Parameters:
        pdf_path (str): The file path to the PDF document to be opened and read.

    Returns:
        list[dict]: A list of dictionaries, each containing the page number
        (adjusted), character count, word count, sentence count, token count, and the extracted text
        for each page.
    """
    doc = fitz.open(pdf_path)  # open a document
    pages_and_texts = []
    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages
        text = page.get_text()  # get plain text encoded as UTF-8
        text = text_formatter(text)
        pages_and_texts.append({"page_number": page_number - 41,  # adjust page numbers since our PDF starts on page 42
                                "page_char_count": len(text),
                                "page_word_count": len(text.split(" ")),
                                "page_sentence_count_raw": len(text.split(". ")),
                                "page_token_count": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
                                "text": text})
    return pages_and_texts

pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)
pages_and_texts[:2]

len(pages_and_texts)

from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=20)

from langchain.schema import Document

# Convert pages_and_texts to a list of Document objects
documents = [
    Document(page_content=item["text"], metadata={"page_number": item["page_number"]})
    for item in pages_and_texts
]

# Now use the documents list with the splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=20)
text_chunks = text_splitter.split_documents(documents)

#Step 05: Split the Extracted Data into Text Chunks
#text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=20)

#text_chunks = text_splitter.split_documents(base_docs)

len(text_chunks)

#get the third chunk
text_chunks[2]

#torch.cuda.empty_cache() # Clear the GPU cache

#Step 06:Downlaod the Embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

#Step 08: Create Embeddings for each of the Text Chunk
vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)

print(vector_store)

!huggingface-cli login

# Load the model on the appropriate device
model_4bit = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    quantization_config=quantization_config,
    token="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",
    #timeout=60
).to(device)  # Move model to the device

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1", use_auth_token=True)

#Model details
model_4bit

print(tokenizer)

pipeline_inst = pipeline(
        "text-generation",
        model=model_4bit,
        tokenizer=tokenizer,
        use_cache=True,
        device_map="auto",
        max_length=10000,
        do_sample=True,
        top_k=5,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
)

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    quantization_config=quantization_config,
    token="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", # Add this line
    #timeout=60
)

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1", use_auth_token=True) # Pass use_auth_token=True here as well

import pathlib
import textwrap
from IPython.display import display
from IPython.display import Markdown



def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

query = "What is tax compliance?"
search = vector_store.similarity_search(query)

to_markdown(search[0].page_content)

"""Retriever"""

retriever = vector_store.as_retriever(
    search_kwargs={'k': 5}
)
#Get top 5 similarities

retriever.get_relevant_documents(query)

"""RAG CHAIN"""

from langchain.llms import HuggingFacePipeline # Import HuggingFacePipeline
llm = HuggingFacePipeline(pipeline=pipeline_inst)

response = llm("What is tax compliance?")
print(response)

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""
#OUTPUT IS WHAT YOU GET FROM MODEL

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever,  "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

response = rag_chain.invoke("What is tax compliance?")

to_markdown(response)

from langchain.llms import HuggingFacePipeline

retriever = vector_store.as_retriever(search_kwargs={'k': 5})

# 2. Define the prompt template (context + query)
prompt_template = """
<|context|>
You are an AI assistant that follows instructions extremely well.
Please be truthful and give direct answers based on the provided context.
</s>
<|user|>
{query}
</s>
<|assistant|>
{context}
"""

llm = HuggingFacePipeline(pipeline=pipeline_inst)

# 4. Build the RAG Chain
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}  # Pass query to the retriever
    | ChatPromptTemplate.from_messages([("system", "You are a helpful assistant."),
                                       ("user", "{query}")])  # Format the query
    | llm  # Use the LLM to generate a response
    | StrOutputParser()  # Parse the output as a string
)

# Example query
query = "What is tax compliance?"

# Execute the RAG chain
response = rag_chain.invoke(query)



# Print the final response
print("Response from Model:", response)

rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}  # Pass query to the retriever
    | ChatPromptTemplate.from_messages([("system", "You are a helpful assistant."),
                                       ("user", "{query}")])  # Format the query
    | llm  # Use the LLM to generate a response
    | StrOutputParser()  # Parse the output as a string
)

response = rag_chain.invoke(query)

# Print the final response
print("Response from Model:", response)

# Assume y_true is a list of binary values (1 for relevant, 0 for irrelevant)
# This is just an example, replace with your actual ground truth data
y_true = [1, 0, 1, 1, 0, 0, 0, 1, 0, 1]

# Now you can calculate precision at K
precision_at_k = sum(y_true[:5]) / 5  # Calculate precision for top 5 results
print(f"Precision at K: {precision_at_k}")

precision_at_k = sum(y_true[:5]) / 5
print(f"Precision at K: {precision_at_k}")

import pandas as pd

def measure_ground_truth(query, retrieved_documents):
    """
    Measures ground truth relevance (y_true) for retrieved documents using manual assessment.

    Args:
        query (str): The search query.
        retrieved_documents (list): A list of retrieved documents (text or metadata).

    Returns:
        list: A list of binary relevance scores (0 or 1) representing y_true.
    """

    relevance_scores = []

    # Create a DataFrame to store document information
    df = pd.DataFrame({'Document': retrieved_documents})

    # Display each document for assessment
    for index, row in df.iterrows():
        print(f"\nQuery: {query}")
        print(f"Document {index + 1}:\n{row['Document']}\n")
        while True:
            relevance = input("Is this document relevant to the query? (1 for Yes, 0 for No): ")
            if relevance in ['0', '1']:
                relevance_scores.append(int(relevance))
                break
            else:
                print("Invalid input. Please enter 1 for relevant or 0 for irrelevant.")

    return relevance_scores

# Example usage:
query = "benefits of exercise"  # Your search query
retrieved_documents = [
    "Document 1: Discusses the various health benefits of exercise.",
    "Document 2: Focuses on different types of exercise equipment.",
    "Document 3: Explains how to create an exercise routine."
]  # Your retrieved documents (replace with actual content)

y_true = measure_ground_truth(query, retrieved_documents)
print(f"\nGround truth relevance (y_true): {y_true}")